{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.3: Naive word2vec (40 points)\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch and code from your previous task.\n",
    "\n",
    "## Results of this task: (30 points)\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    "\n",
    "## Extra questions: (10 points)\n",
    " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
    " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "np.random.seed(4242)\n",
    "random.seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CBOWBatcher:\n",
    "    THRESHOLD = 5\n",
    "    def __init__(self, dataset, window_size=2, threshold=THRESHOLD):\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        c = Counter(dataset)\n",
    "        # all the words we have plus <UNK> token for rare words\n",
    "        unique = {w for w in dataset if c[w] > self.threshold}\n",
    "        self.word2ind = {w: i for i, w in enumerate(unique)}\n",
    "        self.word2ind[UNK_TOKEN] = len(self.word2ind)\n",
    "        self.ind2word = {i: w for w, i in self.word2ind.items()}\n",
    "        # We need to store only the numbers of the words here, as we have their numbers already\n",
    "        # we create a padded array for tokens to process all the words from corpus\n",
    "        # remove all the uncommon words here\n",
    "        self.tokens = ([self.word2ind[UNK_TOKEN]] * window_size) +\\\n",
    "            [self.word2ind.get(w, self.word2ind[UNK_TOKEN]) for w in dataset] +\\\n",
    "            ([self.word2ind[UNK_TOKEN]] * window_size)\n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        assert self.vocab_size == len(self.word2ind)\n",
    "        assert all(t < self.vocab_size for t in self.tokens)\n",
    "        pprint(f'Corpus size: {len(dataset)}')\n",
    "        pprint(f'Actual count of words used: {self.vocab_size}')\n",
    "        pprint(f'{len(dataset)} words in dataset tokenized to {len(self.tokens)} tokens')\n",
    "\n",
    "    def get_batch(self, batch_size=512):\n",
    "        X = [None] * batch_size\n",
    "        y = [None] * batch_size\n",
    "        current = 0\n",
    "        for start in np.random.permutation(range(len(self.tokens) - 2 * window_size)):\n",
    "            center = start + window_size\n",
    "            X[current] = [self.tokens[i]\n",
    "                          for i in range(center - window_size, center + window_size + 1) if i != center]\n",
    "            y[current] = self.tokens[center]\n",
    "            current += 1\n",
    "            if current == batch_size:\n",
    "                # We need the generator, so only `yield ` is an option here\n",
    "                yield torch.from_numpy(np.asarray(X)).to(device=device),\\\n",
    "                      torch.from_numpy(np.asarray(y)).to(device=device)\n",
    "                # clean the buffer after we yielded it and we got back our process here\n",
    "                X = [None] * batch_size\n",
    "                y = [None] * batch_size\n",
    "                current = 0\n",
    "        if current:\n",
    "            # if batch didn't get to the full size but the corpus ended\n",
    "            yield torch.from_numpy(np.asarray(X[:current])).to(device=device),\\\n",
    "                  torch.from_numpy(np.asarray(y[:current])).to(device=device)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Corpus size: 17005207'\n",
      "'Actual count of words used: 58113'\n",
      "'17005207 words in dataset tokenized to 17005211 tokens'\n"
     ]
    }
   ],
   "source": [
    "test8_Data = Path.cwd() / 'text8'\n",
    "with test8_Data.open() as f:\n",
    "    # 1. simple cleaning: lowering all the words\n",
    "    text8 = [a.lower() for line in f for a in line.split()]\n",
    "    batcher = CBOWBatcher(text8, threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWW2V(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size=256, window=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size * window * 2)\n",
    "        self.W = nn.Linear(embedding_size * window * 2, hidden_size)\n",
    "        nn.init.xavier_normal_(self.W.weight)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.W1 = nn.Linear(hidden_size, vocab_size)\n",
    "        nn.init.xavier_normal_(self.W1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the embedding by indices\n",
    "        x = self.embed(x)\n",
    "        # hidden linear layer\n",
    "        x = self.relu(self.W(x))\n",
    "        # get the predictions\n",
    "        x = self.W1(x)\n",
    "        # we need only 1 word by the given ones\n",
    "        # here we got 4 options, so let's average them\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "def test_CBOWW2V_shapes():\n",
    "    window_size = 2\n",
    "    batch_size = 64\n",
    "    vocab_size = 50\n",
    "    x = torch.zeros((batch_size, window_size * 2), dtype=torch.long)\n",
    "    model = CBOWW2V(vocab_size, 42)\n",
    "    scores = model(x)\n",
    "    assert scores.size() == torch.Size([batch_size, vocab_size]), scores.size()\n",
    "\n",
    "\n",
    "test_CBOWW2V_shapes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "EACH_PRINT = 100\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    for e in range(epochs):\n",
    "        with SummaryWriter() as writer:\n",
    "            t = tqdm(batcher.get_batch(1024), desc=f'Epoch {e}')\n",
    "            for step, (x, y) in enumerate(t):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "                for xx in x:\n",
    "                    if not all(xxx.item() <= batcher.vocab_size for xxx in xx):\n",
    "                        pprint(batcher.tokens)\n",
    "                        pprint(x)\n",
    "                        break\n",
    "\n",
    "                scores = model(x)\n",
    "                check = loss(scores, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                check.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += check.data\n",
    "                average_loss = float(total_loss / (len(x) * (step + 1)))\n",
    "                writer.add_scalar('Loss/train', average_loss, step)\n",
    "                t.set_postfix(loss=total_loss.item())\n",
    "                if not step % EACH_PRINT:\n",
    "                    pprint(f'Iteration {step}, loss = {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02ddf55424441aa986c45c386be8aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 0', max=1.0, style=ProgressStyle(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Iteration 0, loss = 0.0107'\n",
      "'Iteration 100, loss = 0.0104'\n",
      "'Iteration 200, loss = 0.0101'\n",
      "'Iteration 300, loss = 0.0098'\n",
      "'Iteration 400, loss = 0.0096'\n",
      "'Iteration 500, loss = 0.0094'\n",
      "'Iteration 600, loss = 0.0092'\n",
      "'Iteration 700, loss = 0.0091'\n",
      "'Iteration 800, loss = 0.0090'\n",
      "'Iteration 900, loss = 0.0089'\n",
      "'Iteration 1000, loss = 0.0088'\n",
      "'Iteration 1100, loss = 0.0088'\n",
      "'Iteration 1200, loss = 0.0087'\n",
      "'Iteration 1300, loss = 0.0086'\n",
      "'Iteration 1400, loss = 0.0086'\n",
      "'Iteration 1500, loss = 0.0085'\n",
      "'Iteration 1600, loss = 0.0085'\n",
      "'Iteration 1700, loss = 0.0085'\n",
      "'Iteration 1800, loss = 0.0084'\n",
      "'Iteration 1900, loss = 0.0084'\n",
      "'Iteration 2000, loss = 0.0084'\n",
      "'Iteration 2100, loss = 0.0083'\n",
      "'Iteration 2200, loss = 0.0083'\n",
      "'Iteration 2300, loss = 0.0083'\n",
      "'Iteration 2400, loss = 0.0083'\n",
      "'Iteration 2500, loss = 0.0082'\n",
      "'Iteration 2600, loss = 0.0082'\n",
      "'Iteration 2700, loss = 0.0082'\n",
      "'Iteration 2800, loss = 0.0082'\n",
      "'Iteration 2900, loss = 0.0081'\n",
      "'Iteration 3000, loss = 0.0081'\n",
      "'Iteration 3100, loss = 0.0081'\n",
      "'Iteration 3200, loss = 0.0081'\n",
      "'Iteration 3300, loss = 0.0081'\n",
      "'Iteration 3400, loss = 0.0081'\n",
      "'Iteration 3500, loss = 0.0081'\n",
      "'Iteration 3600, loss = 0.0080'\n",
      "'Iteration 3700, loss = 0.0080'\n",
      "'Iteration 3800, loss = 0.0080'\n",
      "'Iteration 3900, loss = 0.0080'\n",
      "'Iteration 4000, loss = 0.0080'\n",
      "'Iteration 4100, loss = 0.0080'\n",
      "'Iteration 4200, loss = 0.0080'\n",
      "'Iteration 4300, loss = 0.0079'\n",
      "'Iteration 4400, loss = 0.0079'\n",
      "'Iteration 4500, loss = 0.0079'\n",
      "'Iteration 4600, loss = 0.0079'\n",
      "'Iteration 4700, loss = 0.0079'\n",
      "'Iteration 4800, loss = 0.0079'\n",
      "'Iteration 4900, loss = 0.0079'\n",
      "'Iteration 5000, loss = 0.0079'\n",
      "'Iteration 5100, loss = 0.0079'\n",
      "'Iteration 5200, loss = 0.0078'\n",
      "'Iteration 5300, loss = 0.0078'\n",
      "'Iteration 5400, loss = 0.0078'\n",
      "'Iteration 5500, loss = 0.0078'\n",
      "'Iteration 5600, loss = 0.0078'\n",
      "'Iteration 5700, loss = 0.0078'\n",
      "'Iteration 5800, loss = 0.0078'\n",
      "'Iteration 5900, loss = 0.0078'\n",
      "'Iteration 6000, loss = 0.0078'\n",
      "'Iteration 6100, loss = 0.0078'\n",
      "'Iteration 6200, loss = 0.0078'\n",
      "'Iteration 6300, loss = 0.0078'\n",
      "'Iteration 6400, loss = 0.0077'\n",
      "'Iteration 6500, loss = 0.0077'\n",
      "'Iteration 6600, loss = 0.0077'\n",
      "'Iteration 6700, loss = 0.0077'\n",
      "'Iteration 6800, loss = 0.0077'\n",
      "'Iteration 6900, loss = 0.0077'\n",
      "'Iteration 7000, loss = 0.0077'\n",
      "'Iteration 7100, loss = 0.0077'\n",
      "'Iteration 7200, loss = 0.0077'\n",
      "'Iteration 7300, loss = 0.0077'\n",
      "'Iteration 7400, loss = 0.0077'\n",
      "'Iteration 7500, loss = 0.0077'\n",
      "'Iteration 7600, loss = 0.0077'\n",
      "'Iteration 7700, loss = 0.0077'\n",
      "'Iteration 7800, loss = 0.0076'\n",
      "'Iteration 7900, loss = 0.0076'\n",
      "'Iteration 8000, loss = 0.0076'\n",
      "'Iteration 8100, loss = 0.0076'\n",
      "'Iteration 8200, loss = 0.0076'\n",
      "'Iteration 8300, loss = 0.0076'\n",
      "'Iteration 8400, loss = 0.0076'\n",
      "'Iteration 8500, loss = 0.0076'\n",
      "'Iteration 8600, loss = 0.0076'\n",
      "'Iteration 8700, loss = 0.0076'\n",
      "'Iteration 8800, loss = 0.0076'\n",
      "'Iteration 8900, loss = 0.0076'\n",
      "'Iteration 9000, loss = 0.0076'\n",
      "'Iteration 9100, loss = 0.0076'\n",
      "'Iteration 9200, loss = 0.0076'\n",
      "'Iteration 9300, loss = 0.0076'\n",
      "'Iteration 9400, loss = 0.0076'\n",
      "'Iteration 9500, loss = 0.0075'\n",
      "'Iteration 9600, loss = 0.0075'\n",
      "'Iteration 9700, loss = 0.0075'\n",
      "'Iteration 9800, loss = 0.0075'\n",
      "'Iteration 9900, loss = 0.0075'\n",
      "'Iteration 10000, loss = 0.0075'\n",
      "'Iteration 10100, loss = 0.0075'\n",
      "'Iteration 10200, loss = 0.0075'\n",
      "'Iteration 10300, loss = 0.0075'\n",
      "'Iteration 10400, loss = 0.0075'\n",
      "'Iteration 10500, loss = 0.0075'\n",
      "'Iteration 10600, loss = 0.0075'\n",
      "'Iteration 10700, loss = 0.0075'\n",
      "'Iteration 10800, loss = 0.0075'\n",
      "'Iteration 10900, loss = 0.0075'\n",
      "'Iteration 11000, loss = 0.0075'\n",
      "'Iteration 11100, loss = 0.0075'\n",
      "'Iteration 11200, loss = 0.0075'\n",
      "'Iteration 11300, loss = 0.0075'\n",
      "'Iteration 11400, loss = 0.0075'\n",
      "'Iteration 11500, loss = 0.0075'\n",
      "'Iteration 11600, loss = 0.0075'\n",
      "'Iteration 11700, loss = 0.0074'\n",
      "'Iteration 11800, loss = 0.0074'\n",
      "'Iteration 11900, loss = 0.0074'\n",
      "'Iteration 12000, loss = 0.0074'\n",
      "'Iteration 12100, loss = 0.0074'\n",
      "'Iteration 12200, loss = 0.0074'\n",
      "'Iteration 12300, loss = 0.0074'\n",
      "'Iteration 12400, loss = 0.0074'\n",
      "'Iteration 12500, loss = 0.0074'\n",
      "'Iteration 12600, loss = 0.0074'\n",
      "'Iteration 12700, loss = 0.0074'\n",
      "'Iteration 12800, loss = 0.0074'\n",
      "'Iteration 12900, loss = 0.0074'\n",
      "'Iteration 13000, loss = 0.0074'\n",
      "'Iteration 13100, loss = 0.0074'\n",
      "'Iteration 13200, loss = 0.0074'\n",
      "'Iteration 13300, loss = 0.0074'\n",
      "'Iteration 13400, loss = 0.0074'\n",
      "'Iteration 13500, loss = 0.0074'\n",
      "'Iteration 13600, loss = 0.0074'\n",
      "'Iteration 13700, loss = 0.0074'\n",
      "'Iteration 13800, loss = 0.0074'\n",
      "'Iteration 13900, loss = 0.0074'\n",
      "'Iteration 14000, loss = 0.0074'\n",
      "'Iteration 14100, loss = 0.0074'\n",
      "'Iteration 14200, loss = 0.0074'\n",
      "'Iteration 14300, loss = 0.0074'\n",
      "'Iteration 14400, loss = 0.0073'\n",
      "'Iteration 14500, loss = 0.0073'\n",
      "'Iteration 14600, loss = 0.0073'\n",
      "'Iteration 14700, loss = 0.0073'\n",
      "'Iteration 14800, loss = 0.0073'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7311b9f717c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mASGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-42c49fa5d5f3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxxx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                         \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-42c49fa5d5f3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mxx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxxx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                         \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pprint(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "embedding_size = 222\n",
    "window_size = 2\n",
    "model = CBOWW2V(batcher.vocab_size, embedding_size)\n",
    "model = model.to(device=device)\n",
    "optimizer = optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
