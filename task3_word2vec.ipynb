{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.3: Naive word2vec (40 points)\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch and code from your previous task.\n",
    "\n",
    "## Results of this task: (30 points)\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    "\n",
    "## Extra questions: (10 points)\n",
    " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
    " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "np.random.seed(4242)\n",
    "random.seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CBOWBatcher:\n",
    "    THRESHOLD = 5\n",
    "    def __init__(self, dataset, window_size=2, threshold=THRESHOLD):\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        c = Counter(dataset)\n",
    "        # all the words we have plus <UNK> token for rare words\n",
    "        unique = {w for w in dataset if c[w] > self.threshold}\n",
    "        self.word2ind = {w: i for i, w in enumerate(unique)}\n",
    "        self.word2ind[UNK_TOKEN] = len(self.word2ind)\n",
    "        self.ind2word = {i: w for w, i in self.word2ind.items()}\n",
    "        # We need to store only the numbers of the words here, as we have their numbers already\n",
    "        # we create a padded array for tokens to process all the words from corpus\n",
    "        # remove all the uncommon words here\n",
    "        self.tokens = ([self.word2ind[UNK_TOKEN]] * window_size) +\\\n",
    "            [self.word2ind.get(w, self.word2ind[UNK_TOKEN]) for w in dataset] +\\\n",
    "            ([self.word2ind[UNK_TOKEN]] * window_size)\n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        assert self.vocab_size == len(self.word2ind)\n",
    "        assert all(t < self.vocab_size for t in self.tokens)\n",
    "        pprint(f'Corpus size: {len(dataset)}')\n",
    "        pprint(f'Actual count of words used: {self.vocab_size}')\n",
    "        pprint(f'{len(dataset)} words in dataset tokenized to {len(self.tokens)} tokens')\n",
    "\n",
    "    def get_batch(self, batch_size=512):\n",
    "        X = [None] * batch_size\n",
    "        y = [None] * batch_size\n",
    "        current = 0\n",
    "        for start in np.random.permutation(range(len(self.tokens) - 2 * window_size)):\n",
    "            center = start + window_size\n",
    "            X[current] = [self.tokens[i]\n",
    "                          for i in range(center - window_size, center + window_size + 1) if i != center]\n",
    "            y[current] = self.tokens[center]\n",
    "            current += 1\n",
    "            if current == batch_size:\n",
    "                # We need the generator, so only `yield ` is an option here\n",
    "                yield torch.from_numpy(np.asarray(X)).to(device=device),\\\n",
    "                      torch.from_numpy(np.asarray(y)).to(device=device)\n",
    "                # clean the buffer after we yielded it and we got back our process here\n",
    "                X = [None] * batch_size\n",
    "                y = [None] * batch_size\n",
    "                current = 0\n",
    "        if current:\n",
    "            # if batch didn't get to the full size but the corpus ended\n",
    "            yield torch.from_numpy(np.asarray(X[:current])).to(device=device),\\\n",
    "                  torch.from_numpy(np.asarray(y[:current])).to(device=device)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Corpus size: 17005207'\n",
      "'Actual count of words used: 58113'\n",
      "'17005207 words in dataset tokenized to 17005211 tokens'\n"
     ]
    }
   ],
   "source": [
    "test8_Data = Path.cwd() / 'text8'\n",
    "with test8_Data.open() as f:\n",
    "    # 1. simple cleaning: lowering all the words\n",
    "    text8 = [a.lower() for line in f for a in line.split()]\n",
    "    batcher = CBOWBatcher(text8, threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWW2V(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size=256, window=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size * window * 2)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.W1 = nn.Linear(embedding_size * window * 2, vocab_size)\n",
    "        nn.init.xavier_normal_(self.W1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the embedding by indices\n",
    "        x = self.embed(x)\n",
    "        # hidden linear layer\n",
    "        x = self.relu(x)\n",
    "        # get the predictions\n",
    "        x = self.W1(x)\n",
    "        # we need only 1 word by the given ones\n",
    "        # here we got 4 options, so let's average them\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "def test_CBOWW2V_shapes():\n",
    "    window_size = 2\n",
    "    batch_size = 64\n",
    "    vocab_size = 50\n",
    "    x = torch.zeros((batch_size, window_size * 2), dtype=torch.long)\n",
    "    model = CBOWW2V(vocab_size, 42)\n",
    "    scores = model(x)\n",
    "    assert scores.size() == torch.Size([batch_size, vocab_size]), scores.size()\n",
    "\n",
    "\n",
    "test_CBOWW2V_shapes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "EACH_PRINT = 100\n",
    "writer = SummaryWriter() \n",
    "def train_model(model, optimizer, epochs=1, max_steps=None):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        t = tqdm(batcher.get_batch(1024), desc=f'Epoch {e}')\n",
    "        for step, (x, y) in enumerate(t):\n",
    "            if step > max_steps:\n",
    "                break\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=torch.long)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            check = loss(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            check.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += check.data\n",
    "            average_loss = float(total_loss / (step + 1))\n",
    "            writer.add_scalar('Current loss/train', check.data, step)\n",
    "            writer.add_scalar('Total loss/train', total_loss, step)\n",
    "            writer.add_scalar('Average loss/train', average_loss, step)\n",
    "            t.set_postfix(loss=check.data)\n",
    "            if not step % EACH_PRINT:\n",
    "                pprint(f'Iteration {step}, current loss = {check.data:.4f}, average loss = {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772f894d29224a70aa559f1816b9a69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Epoch 0', max=1.0, style=ProgressStyle(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Iteration 0, loss = 10.9621'\n",
      "'Iteration 100, loss = 8.4109'\n",
      "'Iteration 200, loss = 8.2269'\n",
      "'Iteration 300, loss = 9.8921'\n",
      "'Iteration 400, loss = 10.7657'\n",
      "'Iteration 500, loss = 7.2920'\n",
      "'Iteration 600, loss = 7.5492'\n",
      "'Iteration 700, loss = 8.0928'\n",
      "'Iteration 800, loss = 10.1871'\n",
      "'Iteration 900, loss = 7.1046'\n",
      "'Iteration 1000, loss = 9.2898'\n",
      "'Iteration 1100, loss = 6.7658'\n",
      "'Iteration 1200, loss = 7.3498'\n",
      "'Iteration 1300, loss = 7.7469'\n",
      "'Iteration 1400, loss = 8.1395'\n",
      "'Iteration 1500, loss = 7.5857'\n",
      "'Iteration 1600, loss = 6.9428'\n",
      "'Iteration 1700, loss = 7.1464'\n",
      "'Iteration 1800, loss = 7.0798'\n",
      "'Iteration 1900, loss = 6.7013'\n",
      "'Iteration 2000, loss = 7.1450'\n",
      "'Iteration 2100, loss = 6.9293'\n",
      "'Iteration 2200, loss = 7.1913'\n",
      "'Iteration 2300, loss = 6.7560'\n",
      "'Iteration 2400, loss = 6.8238'\n",
      "'Iteration 2500, loss = 6.8997'\n",
      "'Iteration 2600, loss = 6.3742'\n",
      "'Iteration 2700, loss = 6.6186'\n",
      "'Iteration 2800, loss = 6.8392'\n",
      "'Iteration 2900, loss = 7.0350'\n",
      "'Iteration 3000, loss = 6.7030'\n",
      "'Iteration 3100, loss = 6.9854'\n",
      "'Iteration 3200, loss = 6.9794'\n",
      "'Iteration 3300, loss = 6.7175'\n",
      "'Iteration 3400, loss = 6.7716'\n",
      "'Iteration 3500, loss = 6.5883'\n",
      "'Iteration 3600, loss = 6.6971'\n",
      "'Iteration 3700, loss = 6.8767'\n",
      "'Iteration 3800, loss = 6.8322'\n",
      "'Iteration 3900, loss = 6.6664'\n",
      "'Iteration 4000, loss = 6.6641'\n",
      "'Iteration 4100, loss = 6.8012'\n",
      "'Iteration 4200, loss = 6.7694'\n",
      "'Iteration 4300, loss = 6.6623'\n",
      "'Iteration 4400, loss = 6.5419'\n",
      "'Iteration 4500, loss = 6.7121'\n",
      "'Iteration 4600, loss = 6.6705'\n",
      "'Iteration 4700, loss = 6.7293'\n",
      "'Iteration 4800, loss = 6.5689'\n",
      "'Iteration 4900, loss = 6.6791'\n",
      "'Iteration 5000, loss = 6.5549'\n",
      "'Iteration 5100, loss = 6.6761'\n",
      "'Iteration 5200, loss = 6.3707'\n",
      "'Iteration 5300, loss = 6.7611'\n",
      "'Iteration 5400, loss = 6.4771'\n",
      "'Iteration 5500, loss = 6.7754'\n",
      "'Iteration 5600, loss = 6.6897'\n",
      "'Iteration 5700, loss = 6.5328'\n",
      "'Iteration 5800, loss = 6.4442'\n",
      "'Iteration 5900, loss = 6.5834'\n",
      "'Iteration 6000, loss = 6.7507'\n",
      "'Iteration 6100, loss = 6.8966'\n",
      "'Iteration 6200, loss = 6.6605'\n",
      "'Iteration 6300, loss = 6.5554'\n",
      "'Iteration 6400, loss = 6.7305'\n",
      "'Iteration 6500, loss = 6.5343'\n",
      "'Iteration 6600, loss = 6.5842'\n",
      "'Iteration 6700, loss = 6.7499'\n",
      "'Iteration 6800, loss = 6.7528'\n",
      "'Iteration 6900, loss = 6.5310'\n",
      "'Iteration 7000, loss = 6.7640'\n",
      "'Iteration 7100, loss = 6.6587'\n",
      "'Iteration 7200, loss = 6.7005'\n",
      "'Iteration 7300, loss = 6.7409'\n",
      "'Iteration 7400, loss = 6.8085'\n",
      "'Iteration 7500, loss = 6.6462'\n",
      "'Iteration 7600, loss = 6.7993'\n",
      "'Iteration 7700, loss = 6.6391'\n",
      "'Iteration 7800, loss = 6.5319'\n",
      "'Iteration 7900, loss = 6.8201'\n",
      "'Iteration 8000, loss = 6.5658'\n",
      "'Iteration 8100, loss = 6.5004'\n",
      "'Iteration 8200, loss = 6.6328'\n",
      "'Iteration 8300, loss = 6.5759'\n",
      "'Iteration 8400, loss = 6.5485'\n",
      "'Iteration 8500, loss = 6.7696'\n",
      "'Iteration 8600, loss = 6.6458'\n",
      "'Iteration 8700, loss = 6.6876'\n",
      "'Iteration 8800, loss = 6.5332'\n",
      "'Iteration 8900, loss = 6.6601'\n",
      "'Iteration 9000, loss = 6.7633'\n",
      "'Iteration 9100, loss = 6.8278'\n",
      "'Iteration 9200, loss = 6.8368'\n",
      "'Iteration 9300, loss = 6.6446'\n",
      "'Iteration 9400, loss = 6.5890'\n",
      "'Iteration 9500, loss = 6.7827'\n",
      "'Iteration 9600, loss = 6.7259'\n",
      "'Iteration 9700, loss = 6.5923'\n",
      "'Iteration 9800, loss = 6.6287'\n",
      "'Iteration 9900, loss = 6.7946'\n",
      "'Iteration 10000, loss = 6.7943'\n",
      "'Iteration 10100, loss = 6.8418'\n",
      "'Iteration 10200, loss = 6.6815'\n",
      "'Iteration 10300, loss = 6.6181'\n",
      "'Iteration 10400, loss = 6.6001'\n",
      "'Iteration 10500, loss = 6.7924'\n",
      "'Iteration 10600, loss = 6.6970'\n",
      "'Iteration 10700, loss = 6.8504'\n",
      "'Iteration 10800, loss = 6.7210'\n",
      "'Iteration 10900, loss = 6.8634'\n",
      "'Iteration 11000, loss = 6.5604'\n",
      "'Iteration 11100, loss = 6.6821'\n",
      "'Iteration 11200, loss = 6.8213'\n",
      "'Iteration 11300, loss = 6.7283'\n",
      "'Iteration 11400, loss = 6.5531'\n",
      "'Iteration 11500, loss = 6.8268'\n",
      "'Iteration 11600, loss = 6.6714'\n",
      "'Iteration 11700, loss = 6.8859'\n",
      "'Iteration 11800, loss = 6.6786'\n",
      "'Iteration 11900, loss = 6.7301'\n",
      "'Iteration 12000, loss = 6.6935'\n",
      "'Iteration 12100, loss = 6.7047'\n",
      "'Iteration 12200, loss = 6.6332'\n",
      "'Iteration 12300, loss = 6.7997'\n",
      "'Iteration 12400, loss = 6.6927'\n",
      "'Iteration 12500, loss = 6.6301'\n",
      "'Iteration 12600, loss = 6.8412'\n",
      "'Iteration 12700, loss = 6.9122'\n",
      "'Iteration 12800, loss = 6.7355'\n",
      "'Iteration 12900, loss = 6.7954'\n",
      "'Iteration 13000, loss = 6.7939'\n",
      "'Iteration 13100, loss = 6.5007'\n",
      "'Iteration 13200, loss = 6.8181'\n",
      "'Iteration 13300, loss = 6.8194'\n",
      "'Iteration 13400, loss = 6.8346'\n",
      "'Iteration 13500, loss = 6.6754'\n",
      "'Iteration 13600, loss = 6.8282'\n",
      "'Iteration 13700, loss = 6.7295'\n",
      "'Iteration 13800, loss = 6.7006'\n",
      "'Iteration 13900, loss = 6.7225'\n",
      "'Iteration 14000, loss = 6.6893'\n",
      "'Iteration 14100, loss = 6.6878'\n",
      "'Iteration 14200, loss = 6.9039'\n",
      "'Iteration 14300, loss = 6.6455'\n",
      "'Iteration 14400, loss = 6.7878'\n",
      "'Iteration 14500, loss = 6.8023'\n",
      "'Iteration 14600, loss = 6.6824'\n",
      "'Iteration 14700, loss = 6.7668'\n",
      "'Iteration 14800, loss = 6.6980'\n",
      "'Iteration 14900, loss = 6.4648'\n",
      "'Iteration 15000, loss = 6.6510'\n",
      "'Iteration 15100, loss = 6.6783'\n",
      "'Iteration 15200, loss = 6.5863'\n",
      "'Iteration 15300, loss = 6.9528'\n",
      "'Iteration 15400, loss = 6.6279'\n",
      "'Iteration 15500, loss = 6.4893'\n",
      "'Iteration 15600, loss = 6.6614'\n",
      "'Iteration 15700, loss = 6.7651'\n",
      "'Iteration 15800, loss = 6.5469'\n",
      "'Iteration 15900, loss = 6.7969'\n",
      "'Iteration 16000, loss = 6.6208'\n",
      "'Iteration 16100, loss = 6.7578'\n",
      "'Iteration 16200, loss = 6.7106'\n",
      "'Iteration 16300, loss = 6.5320'\n",
      "'Iteration 16400, loss = 6.5212'\n",
      "'Iteration 16500, loss = 6.8661'\n",
      "'Iteration 16600, loss = 6.8594'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint(device)\n",
    "\n",
    "learning_rate = 1.568\n",
    "embedding_size = 222\n",
    "window_size = 2\n",
    "model = CBOWW2V(batcher.vocab_size, embedding_size)\n",
    "model = model.to(device=device)\n",
    "optimizer = optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWW2V(\n",
       "  (embed): Embedding(58113, 888)\n",
       "  (relu): ReLU()\n",
       "  (W1): Linear(in_features=888, out_features=58113, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results for the model are:\n",
    "# 16607/? [4:12:24<00:00, 1.10it/s, loss=tensor(6.5550, device='cuda:0')]\n",
    "# This is 1 epoch on the whole corpus\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](imgs/LossGraph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cb61479e1502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCBOWW2V\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0moptimizer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mASGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error"
     ]
    }
   ],
   "source": [
    "pprint(device)\n",
    "\n",
    "learning_rate = 0.1568\n",
    "embedding_size = 222\n",
    "window_size = 2\n",
    "model2 = CBOWW2V(batcher.vocab_size, embedding_size)\n",
    "model2 = model2.to(device=device)\n",
    "optimizer2 = optim.ASGD(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model2, optimizer2, epochs=10, max_steps=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
