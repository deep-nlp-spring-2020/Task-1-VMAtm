{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.3: Naive word2vec (40 points)\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch and code from your previous task.\n",
    "\n",
    "## Results of this task: (30 points)\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    "\n",
    "## Extra questions: (10 points)\n",
    " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
    " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "UNK_TOKEN = '<UNK>'\n",
    "\n",
    "np.random.seed(4242)\n",
    "random.seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CBOWBatcher:\n",
    "    THRESHOLD = 5\n",
    "    def __init__(self, dataset, window_size=2, threshold=THRESHOLD):\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        c = Counter(dataset)\n",
    "        # all the words we have plus <UNK> token for rare words\n",
    "        unique = {w for w in dataset if c[w] > self.threshold}\n",
    "        self.word2ind = {w: i for i, w in enumerate(unique)}\n",
    "        self.word2ind[UNK_TOKEN] = len(self.word2ind)\n",
    "        self.ind2word = {i: w for w, i in self.word2ind.items()}\n",
    "        # We need to store only the numbers of the words here, as we have their numbers already\n",
    "        # we create a padded array for tokens to process all the words from corpus\n",
    "        # remove all the uncommon words here\n",
    "        self.tokens = ([self.word2ind[UNK_TOKEN]] * window_size) +\\\n",
    "            [self.word2ind.get(w, self.word2ind[UNK_TOKEN]) for w in dataset] +\\\n",
    "            ([self.word2ind[UNK_TOKEN]] * window_size)\n",
    "        self.vocab_size = len(set(self.tokens))\n",
    "        assert self.vocab_size == len(self.word2ind)\n",
    "        assert all(t < self.vocab_size for t in self.tokens)\n",
    "        pprint(f'Corpus size: {len(dataset)}')\n",
    "        pprint(f'Actual count of words used: {self.vocab_size}')\n",
    "        pprint(f'{len(dataset)} words in dataset tokenized to {len(self.tokens)} tokens')\n",
    "\n",
    "    def get_batch(self, batch_size=512):\n",
    "        X = [None] * batch_size\n",
    "        y = [None] * batch_size\n",
    "        current = 0\n",
    "        for start in np.random.permutation(range(len(self.tokens) - 2 * window_size)):\n",
    "            center = start + window_size\n",
    "            X[current] = [self.tokens[i]\n",
    "                          for i in range(center - window_size, center + window_size + 1) if i != center]\n",
    "            y[current] = self.tokens[center]\n",
    "            current += 1\n",
    "            if current == batch_size:\n",
    "                # We need the generator, so only `yield ` is an option here\n",
    "                yield torch.from_numpy(np.asarray(X)).to(device=device),\\\n",
    "                      torch.from_numpy(np.asarray(y)).to(device=device)\n",
    "                # clean the buffer after we yielded it and we got back our process here\n",
    "                X = [None] * batch_size\n",
    "                y = [None] * batch_size\n",
    "                current = 0\n",
    "        if current:\n",
    "            # if batch didn't get to the full size but the corpus ended\n",
    "            yield torch.from_numpy(np.asarray(X[:current])).to(device=device),\\\n",
    "                  torch.from_numpy(np.asarray(y[:current])).to(device=device)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Corpus size: 17005207'\n",
      "'Actual count of words used: 58113'\n",
      "'17005207 words in dataset tokenized to 17005211 tokens'\n"
     ]
    }
   ],
   "source": [
    "test8_Data = Path.cwd() / 'text8'\n",
    "with test8_Data.open() as f:\n",
    "    # 1. simple cleaning: lowering all the words\n",
    "    text8 = [a.lower() for line in f for a in line.split()]\n",
    "    batcher = CBOWBatcher(text8, threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWW2V(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size=256, window=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size * window * 2)\n",
    "        self.W = nn.Linear(embedding_size * window * 2, hidden_size)\n",
    "        nn.init.xavier_normal_(self.W.weight)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.W1 = nn.Linear(hidden_size, vocab_size)\n",
    "        nn.init.xavier_normal_(self.W1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the embedding by indices\n",
    "        x = self.embed(x)\n",
    "        # hidden linear layer\n",
    "        x = self.relu(self.W(x))\n",
    "        # get the predictions\n",
    "        x = self.W1(x)\n",
    "        # we need only 1 word by the given ones\n",
    "        # here we got 4 options, so let's average them\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.LongTensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "def test_CBOWW2V_shapes():\n",
    "    window_size = 2\n",
    "    batch_size = 64\n",
    "    vocab_size = 50\n",
    "    x = torch.zeros((batch_size, window_size * 2), dtype=torch.long)\n",
    "    model = CBOWW2V(vocab_size, 42)\n",
    "    scores = model(x)\n",
    "    assert scores.size() == torch.Size([batch_size, vocab_size]), scores.size()\n",
    "\n",
    "\n",
    "test_CBOWW2V_shapes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "EACH_PRINT = 100\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    for e in range(epochs):\n",
    "        with SummaryWriter() as writer:\n",
    "            for t, (x, y) in enumerate(batcher.get_batch()):\n",
    "                model.train()\n",
    "                x = x.to(device=device, dtype=torch.long)\n",
    "                y = y.to(device=device, dtype=torch.long)\n",
    "                for xx in x:\n",
    "                    if not all(xxx.item() <= batcher.vocab_size for xxx in xx):\n",
    "                        pprint(batcher.tokens)\n",
    "                        pprint(x)\n",
    "                        break\n",
    "\n",
    "                scores = model(x)\n",
    "                check = loss(scores, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                check.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += check.data\n",
    "                average_loss = float(total_loss / (len(x) * (t + 1)))\n",
    "                writer.add_scalar('Loss/train', average_loss, t)\n",
    "                if not t % EACH_PRINT:\n",
    "                    pprint(f'Iteration {t}, loss = {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n",
      "'Iteration 0, loss = 0.0214'\n"
     ]
    }
   ],
   "source": [
    "pprint(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "embedding_size = 222\n",
    "window_size = 2\n",
    "model = CBOWW2V(batcher.vocab_size, embedding_size)\n",
    "model = model.to(device=device)\n",
    "optimizer = optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
